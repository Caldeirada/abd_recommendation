{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports \n",
    "\n",
    "import os # OS e.g directory structure\n",
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import scipy as sc  # scientific computing\n",
    "import pandas as pd # data processing, file I/O\n",
    "import seaborn as sns  # visualization\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark related imports\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"Oct-2019\"\n",
    "! head -n 5 dataset/2019-Oct.csv\n",
    "! tail -n 5 dataset/2019-Oct.csv\n",
    "! echo \"Nov-2019\"\n",
    "! head -n 5 dataset/2019-Nov.csv\n",
    "! tail -n 5 dataset/2019-Nov.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"abd_recommendation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barPlot(columns, values):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,2,1])\n",
    "    ax.bar(columns, values)\n",
    "    for i, d in enumerate(values):\n",
    "        plt.text(i - int(math.log10(d+1))*0.05, d + 0.01, str(d))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data, reading all csv files into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.csv(\"dataset/2019-Nov-small.csv\", header=\"true\", inferSchema=\"true\", sep=\",\").union(spark.read.csv(\"dataset/2019-Oct-small.csv\", header=\"true\", inferSchema=\"true\", sep=\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.printSchema()\n",
    "sales.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = sales.where(col(\"category_code\").isNull()).groupBy(\"category_id\").count()\n",
    "c = sales.where(col(\"category_code\").isNotNull()).groupBy(\"category_id\").count()\n",
    "a = b.join(c, b.category_id == c.category_id, 'full')\n",
    "a.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count null values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nulls = sales.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in sales.columns])\n",
    "df_nulls.show()\n",
    "barPlot(df_nulls.columns ,[r for r in df_nulls.first()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar nulls\n",
    "\n",
    "df_sales = sales.select(\"event_type\", \"product_id\", \"category_id\", \"category_code\", \"brand\", \"user_id\", \"user_session\").where(col(\"brand\").isNotNull() & col(\"category_code\").isNotNull())\n",
    "df_sales.printSchema()\n",
    "df_sales.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = df_sales.select(\"product_id\").distinct()\n",
    "brands = df_sales.select(\"brand\").distinct()\n",
    "categories = df_sales.select(\"category_id\").distinct()\n",
    "\n",
    "products.write.mode(\"overwrite\").parquet(\"products\")\n",
    "brands.write.mode(\"overwrite\").parquet(\"brands\")\n",
    "categories.write.mode(\"overwrite\").parquet(\"categories\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_session_product = df_sales.select(\"user_session\",\"product_id\").where(col(\"event_type\") == \"purchase\").dropDuplicates().groupBy('user_session').agg(collect_list('product_id').alias('products'))\n",
    "\n",
    "df_user_product = df_sales.where(col(\"event_type\") == \"purchase\").groupBy('user_id').agg(collect_list('product_id').alias('products'))\n",
    "#df_user_product.show(truncate=False)\n",
    "\n",
    "df_user_brand = df_sales.where(col(\"event_type\") == \"purchase\").groupBy('user_id').agg(collect_list('brand').alias('brands'))\n",
    "#df_user_brand.show(truncate=False)\n",
    "\n",
    "df_user_category = df_sales.where(col(\"event_type\") == \"purchase\").groupBy('user_id').agg(collect_list('category_id').alias('categories'))\n",
    "#df_user_category.show(truncate=False)\n",
    "\n",
    "df_user_product_v = df_sales.where(col(\"event_type\") == \"view\").groupBy('user_id').agg(collect_list('product_id').alias('products'))\n",
    "#df_user_product_v.show(truncate=False)\n",
    "\n",
    "df_user_brand_v = df_sales.where(col(\"event_type\") == \"view\").groupBy('user_id').agg(collect_list('brand').alias('brands'))\n",
    "#df_user_brand_v.show(truncate=False)\n",
    "\n",
    "df_user_category_v = df_sales.where(col(\"event_type\") == \"view\").groupBy('user_id').agg(collect_list('category_id').alias('categories'))\n",
    "#df_user_category_v.show(truncate=False)\n",
    "\n",
    "df_user_product.write.mode(\"overwrite\").parquet(\"user_products\")\n",
    "df_user_brand.write.mode(\"overwrite\").parquet(\"user_brands\")\n",
    "df_user_category.write.mode(\"overwrite\").parquet(\"user_categories\")\n",
    "df_user_product_v.write.mode(\"overwrite\").parquet(\"user_products_v\")\n",
    "df_user_brand_v.write.mode(\"overwrite\").parquet(\"user_brands_v\")\n",
    "df_user_category_v.write.mode(\"overwrite\").parquet(\"user_categories_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session_product = df_sales.where(col(\"event_type\") == \"purchase\").groupBy('user_session').agg(collect_list('product_id').alias('products'))\n",
    "#df_session_product.show(truncate=False)\n",
    "\n",
    "df_session_brand = df_sales.where(col(\"event_type\") == \"purchase\").groupBy('user_session').agg(collect_list('brand').alias('brands'))\n",
    "#df_session_brand.show(truncate=False)\n",
    "\n",
    "df_session_category = df_sales.where(col(\"event_type\") == \"purchase\").groupBy('user_session').agg(collect_list('category_id').alias('categories'))\n",
    "#df_session_category.show(truncate=False)\n",
    "\n",
    "df_session_product_v = df_sales.where(col(\"event_type\") == \"view\").groupBy('user_session').agg(collect_list('product_id').alias('products'))\n",
    "#df_session_product_v.show(truncate=False)\n",
    "\n",
    "df_session_brand_v = df_sales.where(col(\"event_type\") == \"view\").groupBy('user_session').agg(collect_list('brand').alias('brands'))\n",
    "#df_session_brand_v.show(truncate=False)\n",
    "\n",
    "df_session_category_v = df_sales.where(col(\"event_type\") == \"view\").groupBy('user_session').agg(collect_list('category_id').alias('categories'))\n",
    "#df_session_category_v.show(truncate=False)\n",
    "\n",
    "df_session_product.write.mode(\"overwrite\").parquet(\"session_products\")\n",
    "df_session_brand.write.mode(\"overwrite\").parquet(\"session_brands\")\n",
    "df_session_category.write.mode(\"overwrite\").parquet(\"session_categories\")\n",
    "df_session_product_v.write.mode(\"overwrite\").parquet(\"session_products_v\")\n",
    "df_session_brand_v.write.mode(\"overwrite\").parquet(\"session_brands_v\")\n",
    "df_session_category_v.write.mode(\"overwrite\").parquet(\"session_categories_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count purchase brands\n",
    "\n",
    "limit = 15\n",
    "df_count_purchase_brands = df_sales.where(col(\"event_type\") == \"purchase\").groupBy(\"brand\").count().orderBy(col(\"count\").desc())\n",
    "df_count_purchase_brands.limit(limit).show()\n",
    "barPlot([col[\"brand\"] for col in df_count_purchase_brands.select(\"brand\").limit(limit).collect()], [row[\"count\"] for row in df_count_purchase_brands.take(limit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count view brands\n",
    "\n",
    "limit = 15\n",
    "df_count_view_brands = df_sales.where(col(\"event_type\") == \"view\").groupBy(\"brand\").count().orderBy(col(\"count\").desc())\n",
    "df_count_view_brands.limit(limit).show()\n",
    "barPlot([col[\"brand\"] for col in df_count_view_brands.select(\"brand\").limit(limit).collect()], [row[\"count\"] for row in df_count_view_brands.take(limit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brands viewed vs brands purchased\n",
    "\n",
    "limit = 15\n",
    "purchase_view_brands = df_count_purchase_brands.select(col(\"brand\").alias(\"brand_p\"), col(\"count\").alias(\"count_p\")).join(df_count_view_brands.select(col(\"brand\").alias(\"brand_v\"), col(\"count\").alias(\"count_v\")), col(\"brand_p\") == col(\"brand_v\"), 'full').where(col(\"brand_p\").isNotNull()).withColumn(\"views_per_purchase\", col(\"count_v\").cast('int')/col(\"count_p\").cast('int')).orderBy(col(\"count_v\").desc())\n",
    "purchase_view_brands.show()\n",
    "barPlot([col[\"brand_p\"] for col in purchase_view_brands.select(\"brand_p\").limit(limit).collect()], [int(col[\"views_per_purchase\"]) for col in purchase_view_brands.select(\"views_per_purchase\").limit(limit).collect()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produto x purchase\n",
    "\n",
    "limit = 15\n",
    "products_purchased = df_sales.where(col(\"event_type\") == \"purchase\").groupBy(\"product_id\", \"brand\").count().orderBy(col(\"count\").desc())\n",
    "products_purchased.show()\n",
    "barPlot([str(col[\"product_id\"]) for col in products_purchased.select(\"product_id\").limit(limit).collect()], [row[\"count\"] for row in products_purchased.take(limit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produto x view\n",
    "\n",
    "limit = 15\n",
    "products_viewed = df_sales.where(col(\"event_type\") == \"view\").groupBy(\"product_id\", \"brand\").count().orderBy(col(\"count\").desc())\n",
    "products_viewed.show()\n",
    "barPlot([str(col[\"product_id\"]) for col in products_viewed.select(\"product_id\").limit(limit).collect()], [row[\"count\"] for row in products_viewed.take(limit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products viewed vs products purchased\n",
    "\n",
    "limit = 15\n",
    "purchase_view_products = products_purchased.select(col(\"product_id\").alias(\"product_p\"), col(\"count\").alias(\"count_p\")).join(products_viewed.select(col(\"product_id\").alias(\"product_v\"), col(\"count\").alias(\"count_v\")), col(\"product_p\") == col(\"product_v\"), 'full').where(col(\"product_p\").isNotNull()).withColumn(\"views_per_purchase\", col(\"count_v\").cast('int')/col(\"count_p\").cast('int')).orderBy(col(\"count_v\").desc())\n",
    "purchase_view_products.show()\n",
    "barPlot([str(col[\"product_p\"]) for col in purchase_view_products.select(\"product_p\").limit(limit).collect()], [int(col[\"views_per_purchase\"]) for col in purchase_view_products.select(\"views_per_purchase\").limit(limit).collect()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category x purchase\n",
    "\n",
    "limit = 5\n",
    "category_purchased = df_sales.where(col(\"event_type\") == \"purchase\").groupBy(\"category_id\", \"category_code\").count().orderBy(col(\"count\").desc())\n",
    "category_purchased.show(truncate=False)\n",
    "barPlot([str(col[\"category_code\"]) for col in category_purchased.select(\"category_code\").limit(limit).collect()], [row[\"count\"] for row in category_purchased.take(limit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category x view\n",
    "\n",
    "limit = 5\n",
    "category_viewd = df_sales.where(col(\"event_type\") == \"view\").groupBy(\"category_id\", \"category_code\").count().orderBy(col(\"count\").desc())\n",
    "category_viewd.show(truncate=False)\n",
    "barPlot([str(col[\"category_code\"]) for col in category_viewd.select(\"category_code\").limit(limit).collect()], [row[\"count\"] for row in category_viewd.take(limit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories viewed vs categories purchased\n",
    "\n",
    "limit = 5\n",
    "purchase_view_categories = category_purchased.select(col(\"category_code\").alias(\"category_p\"), col(\"count\").alias(\"count_p\")).join(category_viewd.select(col(\"category_code\").alias(\"category_v\"), col(\"count\").alias(\"count_v\")), col(\"category_p\") == col(\"category_v\"), 'full').where(col(\"category_p\").isNotNull()).withColumn(\"views_per_purchase\", col(\"count_v\").cast('int')/col(\"count_p\").cast('int')).orderBy(col(\"count_v\").desc())\n",
    "purchase_view_categories.show()\n",
    "barPlot([str(col[\"category_p\"]) for col in purchase_view_categories.select(\"category_p\").limit(limit).collect()], [int(col[\"views_per_purchase\"]) for col in purchase_view_categories.select(\"views_per_purchase\").limit(limit).collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete nulls (brand and category_code)\n",
    "\n",
    "df_sales = sales.select(\"event_type\", \"product_id\", \"category_id\", \"brand\", \"user_id\", \"user_session\").where(col(\"brand\").isNotNull())\n",
    "df_sales.printSchema()\n",
    "df_sales.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column by joint the value os cols brand and category_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_brand_category_id(b,c):\n",
    "    return b+\".\"+str(c)\n",
    "\n",
    "concat = udf(concat_brand_category_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_new = df_sales.withColumn(\"brand_category\", concat(col(\"brand\"), col(\"category_id\")))\n",
    "\n",
    "df_sales_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "category_code x purchase\n",
    "\n",
    "brands x category_code\n",
    "\n",
    "category_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parquet for product, brand and category\n",
    "\n",
    "df_sales_new.select(\"product_id\").distinct().write.mode(\"overwrite\").parquet(\"products\")\n",
    "df_sales_new.select(\"brand\").distinct().write.mode(\"overwrite\").parquet(\"brands\")\n",
    "df_sales_new.select(\"category_id\").distinct().write.mode(\"overwrite\").parquet(\"categories\")\n",
    "\n",
    "\n",
    "\n",
    "# Create user parquet for purchase and view (_v) events\n",
    "\n",
    "df_sales_new.select(\"user_id\",\"product_id\").where(col(\"event_type\") == \"purchase\").dropDuplicates().groupBy('user_id').agg(collect_list('product_id').alias('items')).write.mode(\"overwrite\").parquet(\"user_products\")\n",
    "df_sales_new.select(\"user_id\",\"brand\").where(col(\"event_type\") == \"purchase\").dropDuplicates().groupBy('user_id').agg(collect_list('brand').alias('items')).write.mode(\"overwrite\").parquet(\"user_brands\")\n",
    "df_sales_new.select(\"user_id\",\"category_id\").where(col(\"event_type\") == \"purchase\").dropDuplicates().groupBy('user_id').agg(collect_list('category_id').alias('items')).write.mode(\"overwrite\").parquet(\"user_categories\")\n",
    "df_sales_new.select(\"user_id\",\"brand_category\").where(col(\"event_type\") == \"purchase\").dropDuplicates().groupBy('user_id').agg(collect_list('brand_category').alias('items')).write.mode(\"overwrite\").parquet(\"user_category_brand\")\n",
    "\n",
    "df_sales_new.select(\"user_id\",\"product_id\").where(col(\"event_type\") == \"view\").dropDuplicates().groupBy('user_id').agg(collect_list('product_id').alias('items')).write.mode(\"overwrite\").parquet(\"user_products_v\")\n",
    "df_sales_new.select(\"user_id\",\"brand\").where(col(\"event_type\") == \"view\").dropDuplicates().groupBy('user_id').agg(collect_list('brand').alias('items')).write.mode(\"overwrite\").parquet(\"user_brands_v\")\n",
    "df_sales_new.select(\"user_id\",\"category_id\").where(col(\"event_type\") == \"view\").dropDuplicates().groupBy('user_id').agg(collect_list('category_id').alias('items')).write.mode(\"overwrite\").parquet(\"user_categories_v\")\n",
    "df_sales_new.select(\"user_id\",\"brand_category\").where(col(\"event_type\") == \"view\").dropDuplicates().groupBy('user_id').agg(collect_list('brand_category').alias('items')).write.mode(\"overwrite\").parquet(\"user_category_brand_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session parquet for purchase and view (_v) events\n",
    "\n",
    "df_sales_new.select(\"user_session\",\"product_id\").where(col(\"event_type\") == \"purchase\").dropDuplicates().groupBy('user_session').agg(collect_list('product_id').alias('items')).write.mode(\"overwrite\").parquet(\"session_products\")\n",
    "df_sales_new.select(\"user_session\",\"brand\").where(col(\"event_type\") == \"purchase\").dropDuplicates().groupBy('user_session').agg(collect_list('brand').alias('items')).write.mode(\"overwrite\").parquet(\"session_brands\")\n",
    "df_sales_new.select(\"user_session\",\"category_id\").where(col(\"event_type\") == \"purchase\").dropDuplicates().groupBy('user_session').agg(collect_list('category_id').alias('items')).write.mode(\"overwrite\").parquet(\"session_categories\")\n",
    "df_sales_new.select(\"user_session\",\"brand_category\").where(col(\"event_type\") == \"purchase\").dropDuplicates().groupBy('user_session').agg(collect_list('brand_category').alias('items')).write.mode(\"overwrite\").parquet(\"session_category_brand\")\n",
    "\n",
    "df_sales_new.select(\"user_session\",\"product_id\").where(col(\"event_type\") == \"view\").dropDuplicates().groupBy('user_session').agg(collect_list('product_id').alias('items')).write.mode(\"overwrite\").parquet(\"session_products_v\")\n",
    "df_sales_new.select(\"user_session\",\"brand\").where(col(\"event_type\") == \"view\").dropDuplicates().groupBy('user_session').agg(collect_list('brand').alias('items')).write.mode(\"overwrite\").parquet(\"session_brands_v\")\n",
    "df_sales_new.select(\"user_session\",\"category_id\").where(col(\"event_type\") == \"view\").dropDuplicates().groupBy('user_session').agg(collect_list('category_id').alias('items')).write.mode(\"overwrite\").parquet(\"session_categories_v\")\n",
    "df_sales_new.select(\"user_session\",\"brand_category\").where(col(\"event_type\") == \"view\").dropDuplicates().groupBy('user_session').agg(collect_list('brand_category').alias('items')).write.mode(\"overwrite\").parquet(\"session_category_brand_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_brand_category_id(s):\n",
    "    return s.split(\".\")[1]\n",
    "\n",
    "split = udf(split_brand_category_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- event_type: string (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- category_id: long (nullable = true)\n |-- brand: string (nullable = true)\n |-- user_id: integer (nullable = true)\n |-- user_session: string (nullable = true)\n |-- brand_category: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_sales_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sales_new = df_sales.withColumn(\"brand_category\", concat(col(\"brand\"), col(\"category_id\")))\n",
    "df_sales_new2 = df_sales_new.withColumn(\"category_id\", split(col(\"brand_category\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifNull(ColIn, ReplaceVal):\n",
    "    return(when(ColIn == \"null\", ReplaceVal).otherwise(ColIn))\n",
    "\n",
    "nvl = udf(ifNull, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o555.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 69) (FosquitoMiGamingLaptop executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor112.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-098d1025a4fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msales\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"category_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"category_code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cat_code\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnvl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"category_code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"uncategorized\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Programas\\Python\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programas\\Python\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programas\\Python\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programas\\Python\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o555.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 69) (FosquitoMiGamingLaptop executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor112.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n"
     ]
    }
   ],
   "source": [
    "a = sales.select(\"category_id\", \"category_code\").distinct().withColumn(\"cat_code\", nvl(col(\"category_code\"), lit(\"uncategorized\")))\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python372jvsc74a57bd0a60d46c41efb414509a1056710712d50c905a250c7a687ab2bbbf0aaa8f3a164",
   "display_name": "Python 3.7.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}